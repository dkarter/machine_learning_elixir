<!-- livebook:{"persist_outputs":true} -->

# Chapter 1

```elixir
Mix.install([
  {:axon, "~> 0.5"},
  {:nx, "~> 0.5"},
  {:explorer, "~> 0.5"},
  {:kino, "~> 0.8"}
])
```

## Section

```elixir
require Explorer.DataFrame, as: DF

# after executing this command, Explorer will download the built-in dataset for this project
# it is a dataframe with 150 rows (examples), and 5 columns (features). The values in each 
# column are considered a Series - Usually it only make sense to run aggregate functions (e.g. avg, sum)
# against the series, not the whole dataset.
#
# The `species` column is a "categorical feature"
#
# Example output:
# #Explorer.DataFrame<
#  Polars[150 x 5]
#  sepal_length f64 [5.1, 4.9, 4.7, 4.6, 5.0, ...]
#  sepal_width f64 [3.5, 3.0, 3.2, 3.1, 3.6, ...]
#  petal_length f64 [1.4, 1.4, 1.3, 1.5, 1.4, ...]
#  petal_width f64 [0.2, 0.2, 0.2, 0.2, 0.2, ...]
#  species string ["Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", ...]
# >
iris = Explorer.Datasets.iris()
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[150 x 5]
  sepal_length f64 [5.1, 4.9, 4.7, 4.6, 5.0, ...]
  sepal_width f64 [3.5, 3.0, 3.2, 3.1, 3.6, ...]
  petal_length f64 [1.4, 1.4, 1.3, 1.5, 1.4, ...]
  petal_width f64 [0.2, 0.2, 0.2, 0.2, 0.2, ...]
  species string ["Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", ...]
>
```

```elixir
# normalize the data using standardization
# `species` does not need to be normalized because it is a categorical feature
cols = ~w(sepal_length sepal_width petal_length petal_width)

# To standardize a series, calculate the mean and variance of the seriesy.
# Then reduce each value in the series by the mean and divide by the variance.
# The output of this expects a tuple with {column_name, values} (which is technically
# a keyword list) and will return a new DataFrame with the new normalized column values.
#
# Note:
# There's a lot of macro magic going on here üßê - it's nice because it saves time, 
# but also a bit too mysterious.
# also `mutate` seems like a bit of a misnomer.. it doesn't actually affect the original
# dataframe. It's more like an update.
normalized_iris =
  DF.mutate(
    iris,
    for col <- across(^cols) do
      {col.name, (col - mean(col)) / variance(col)}
    end
  )
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[150 x 5]
  sepal_length f64 [-1.0840606189132322, -1.3757361217598405, -1.66741162460645,
   -1.8132493760297554, -1.2298983703365365, ...]
  sepal_width f64 [2.3722896125315045, -0.28722789030650403, 0.7765791108287005, 0.2446756102610982,
   2.9041931130991068, ...]
  petal_length f64 [-0.757639168744384, -0.757639168744384, -0.789760671093637, -0.7255176663951308,
   -0.757639168744384, ...]
  petal_width f64 [-1.714701435665471, -1.714701435665471, -1.714701435665471, -1.714701435665471,
   -1.714701435665471, ...]
  species string ["Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", ...]
>
```

```elixir
# convert the `species` into a categorical feature, by telling Explorer to cast that series
# to a category.
#
# Note: after running this, in the output next to the column, istead of `string` it will 
# now show `category`
normalized_iris =
  DF.mutate(
    normalized_iris,
    species: Explorer.Series.cast(species, :category)
  )
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[150 x 5]
  sepal_length f64 [-1.0840606189132322, -1.3757361217598405, -1.66741162460645,
   -1.8132493760297554, -1.2298983703365365, ...]
  sepal_width f64 [2.3722896125315045, -0.28722789030650403, 0.7765791108287005, 0.2446756102610982,
   2.9041931130991068, ...]
  petal_length f64 [-0.757639168744384, -0.757639168744384, -0.789760671093637, -0.7255176663951308,
   -0.757639168744384, ...]
  petal_width f64 [-1.714701435665471, -1.714701435665471, -1.714701435665471, -1.714701435665471,
   -1.714701435665471, ...]
  species category ["Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", "Iris-setosa", ...]
>
```

```elixir
# because the data is ordered, and that does not match what would happen in a production system
# let's shuffle the DataFrame. Also this allows us to split the dataset into a training set and 
# testing set

# Note: this appears to have the same ordering as the book, suggesting that shuffle may be using
# a pre-determined seed ü§î

shuffled_normal_iris = DF.shuffle(normalized_iris)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[150 x 5]
  sepal_length f64 [1.1035056524363374, 1.249343403859643, -1.0840606189132322, -1.521573873183146,
   0.6659923981664237, ...]
  sepal_width f64 [-0.8191313908741062, -2.946745393144513, 3.968000114234309, 0.2446756102610982,
   -1.3510348914417085, ...]
  petal_length f64 [0.27024890643171645, 0.6557069346227542, -0.5970316569981182,
   -0.6933961640458776, 0.4308564181779822, ...]
  petal_width f64 [0.17398839667633606, 1.032483775013521, -1.3713032843305968, -1.714701435665471,
   0.51738654801121, ...]
  species category ["Iris-versicolor", "Iris-virginica", "Iris-setosa", "Iris-setosa",
   "Iris-virginica", ...]
>
```

```elixir
# split the dataset into a training set and testing set
# the model should never see the testing set during training, so that we can use it later
# for validation

train_set = DF.slice(shuffled_normal_iris, 0..119)
test_set = DF.slice(shuffled_normal_iris, 120..149)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[30 x 5]
  sepal_length f64 [0.6659923981664237, -1.8132493760297554, -0.5007096132200132,
   -0.5007096132200132, -0.5007096132200132, ...]
  sepal_width f64 [-2.946745393144513, 1.8403861119639024, -4.010552394279717, 2.3722896125315045,
   -3.4786488937121147, ...]
  petal_length f64 [0.3666134134794761, -0.757639168744384, 0.07751989233619774, -0.789760671093637,
   -0.01884461471156162, ...]
  petal_width f64 [0.51738654801121, -1.5430023599980338, 0.17398839667633606, -1.714701435665471,
   -0.341108830325975, ...]
  species category ["Iris-versicolor", "Iris-setosa", "Iris-versicolor", "Iris-setosa",
   "Iris-versicolor", ...]
>
```

```elixir
# convert the train and test data into a one-hot encoded tensors
feature_cols = ~w(sepal_length sepal_width petal_length petal_width)

# Note: x is a naming convention for features in ML

# Q? what is the "axis: -1" part?
# Q? how does train_set take a list as an index?? macro magic?
# stack the rows of the dataframe into individual entries
# this converts it into a tensor
x_train = Nx.stack(train_set[feature_cols], axis: -1)

# Q? what is Nx.iota? the book calls it magic ü§™, promises it will become clear later in chapter 2
# this converts the species category into a one-hot encoding
y_train =
  train_set["species"]
  |> Nx.stack(axis: -1)
  |> Nx.equal(Nx.iota({1, 3}, axis: -1))

# do the same for the test set
x_test = Nx.stack(test_set[feature_cols], axis: -1)

y_test =
  test_set["species"]
  |> Nx.stack(axis: -1)
  |> Nx.equal(Nx.iota({1, 3}, axis: -1))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8[30][3]
  [
    [0, 1, 0],
    [1, 0, 0],
    [0, 1, 0],
    [1, 0, 0],
    [0, 1, 0],
    [1, 0, 0],
    [1, 0, 0],
    [0, 1, 0],
    [1, 0, 0],
    [1, 0, 0],
    [0, 1, 0],
    [0, 0, 1],
    [0, 0, 1],
    [1, 0, 0],
    [0, 0, 1],
    [1, 0, 0],
    [0, 1, ...],
    ...
  ]
>
```

```elixir
# Now it's time to create and train the model
# There are three steps:
# 1. Define the model (function)
# 2. Create an input pipeline
# 3. Declare and run the training loop

# declare the model
# Q? - what the heck did I just type?
# A:
# - the shape represents the values of each dimension in the data (aka feature cols)
# - 3 - the model transforms the data into three probablities - representing the probability
#   that each example belongs to one of the three categories/classes
model =
  Axon.input("iris_features", shape: {nil, 4})
  |> Axon.dense(3, activation: :softmax)
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"iris_features" => {nil, 4}}
  outputs: "softmax_0"
  nodes: 3
>
```

```elixir
# visualize the model
Axon.Display.as_graph(model, Nx.template({1, 4}, :f32))
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
14[/"iris_features (:input) {1, 4}"/];
15["dense_0 (:dense) {1, 3}"];
16["softmax_0 (:softmax) {1, 3}"];
15 --> 16;
14 --> 15;
```

```elixir
# repeatedly return tuples of the training features and targets respectively
data_stream =
  Stream.repeatedly(fn ->
    {x_train, y_train}
  end)

# the training loop
# 1. Grab inputs from the input pipeline
# 2. Make predictions from inputs
# 3. Determine how good the predictions were
# 4. Update the model based on prediction goodness
# 5. Repeat
trained_model_state =
  model
  #  :categorical_cross_entropy loss function optimized with :sgd or stochastic-gradient descent.
  |> Axon.Loop.trainer(:categorical_cross_entropy, :sgd)
  |> Axon.Loop.metric(:accuracy)
  # run 10 times, pulling 500 frames from the input pipeline
  |> Axon.Loop.run(data_stream, %{}, iterations: 500, epochs: 10)
```

<!-- livebook:{"output":true} -->

```
Epoch: 0, Batch: 450, accuracy: 0.7246500 loss: 0.6196262
Epoch: 1, Batch: 450, accuracy: 0.8786194 loss: 0.4841444
Epoch: 2, Batch: 450, accuracy: 0.9098835 loss: 0.4223212
Epoch: 3, Batch: 450, accuracy: 0.9239621 loss: 0.3826824
Epoch: 4, Batch: 450, accuracy: 0.9348671 loss: 0.3536955
Epoch: 5, Batch: 450, accuracy: 0.9385065 loss: 0.3310152
Epoch: 6, Batch: 450, accuracy: 0.9488579 loss: 0.3125253
Epoch: 7, Batch: 450, accuracy: 0.9500036 loss: 0.2970259
Epoch: 8, Batch: 450, accuracy: 0.9518324 loss: 0.2837669
Epoch: 9, Batch: 450, accuracy: 0.9583363 loss: 0.2722455
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[3]
      [-0.5883989930152893, 1.6622918844223022, -1.0738939046859741]
    >,
    "kernel" => #Nx.Tensor<
      f32[4][3]
      [
        [-1.906885027885437, 0.5639040470123291, 0.9340485334396362],
        [0.9665769934654236, -0.5024322867393494, -0.703886091709137],
        [-0.9946820139884949, -0.7029554843902588, 0.2173878699541092],
        [-1.3911545276641846, -0.741605281829834, 2.106214761734009]
      ]
    >
  }
}
```

```elixir
# evaluate the trained model
data = [{x_test, y_test}]

model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(data, trained_model_state)
```

<!-- livebook:{"output":true} -->

```
Batch: 0, accuracy: 0.9333333
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "accuracy" => #Nx.Tensor<
      f32
      0.9333333373069763
    >
  }
}
```
